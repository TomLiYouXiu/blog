---
title: 你好贾维斯
date: 2024-05-10 10:56:51
tags: 教程
categories: 教程
thumbnail: "https://pic.imgdb.cn/item/663c472f0ea9cb1403e4c285.png"
---

# Hello，Jarvis

![708523c1da6b1f58c8d838d4d20dcac3695af3ed.jpg](https://s3.langlangy.com/langlangyblog/708523c1da6b1f58c8d838d4d20dcac3695af3ed.jpg)
# [视频地址](https://www.bilibili.com/video/BV1vC411j7En)

# 前言

我心中的真正的人工智能的样子？

> 在我心中，理想的人工智能是类似于电影《钢铁侠》中的贾维斯那样的存在。它不仅是一个高效的助手，可以处理各种复杂的任务，而且还能理解和预测我的需求，提供个性化的帮助。它具有自我学习和改进的能力，可以通过与我互动来不断提高其服务质量。更重要的是，这样的人工智能应具备一定的情感理解能力，能够理解人类的情绪，并在与人类的交互中表现出适当的情感反馈。最后，我希望这样的人工智能在满足所有这些功能的同时，还能够严格遵守数据安全和隐私保护的原则，让我在使用的过程中感到安心。

现在人工智能的一大主要问题

> 现在的人工智能（AI）在进行系统级操作方面存在一定的限制。这个问题主要体现在以下几个方面：
>
> 1. **理解和实施复杂任务**：虽然AI可以学习和执行一些特定的、有限的任务，但它们往往在理解和实施复杂、多步骤的任务时遇到困难。例如，AI可能能够理解单个指令，如“打开文件”或“搜索信息”，但可能无法理解或执行更复杂的指令，如“整理这个文件夹并删除所有旧的文件”。
>
> 2. **长期记忆和知识转移**：当前的AI系统通常缺乏有效的长期记忆机制，这意味着它们可能无法记住过去的操作或学习经验，并将这些知识应用到新的任务中。这使得AI难以进行系统级的操作，因为这些操作通常需要对过去的信息进行整合和利用。
>
> 3. **安全性和隐私问题**：AI进行系统级操作可能会涉及到敏感的信息和关键的系统资源。然而，现有的AI技术可能无法确保这些信息和资源的安全，因为它们可能被恶意利用，或者在无意中泄露用户的隐私。
>
> 4. **缺乏通用性**：许多AI系统都是为特定的任务或环境设计的，而不是为广泛的系统级操作设计的。这意味着，尽管这些系统在特定的任务或环境中可能表现良好，但在其他环境或任务中可能表现不佳。
>
> 这些问题都是AI进行系统级操作的主要障碍。然而，随着AI技术的不断发展，我期待这些问题在未来得到解决。

# 项目推荐

GitHub地址：[OpenInterpreter/open-interpreter: A natural language interface for computers (github.com)](https://github.com/OpenInterpreter/open-interpreter)

官网：[The Open Interpreter Project](https://www.openinterpreter.com/)

## 项目简介

why interpreter

> 借助大语言模型，Open Interpreter 可在本地运行代码（Python、Javascript、Shell 等），帮你执行各种任务。
>
> 它可以在本机调用 GPT-4, GPT-3.5，或者开源免费的 CodeLlama 模型。
>
> 安装以后，每次只要打开终端，输入 “interpreter” 这个命令以后，就可以打开类似于 ChatGPT 的聊天界面。
>
> 所不同的是，它并不像代码解释器那样是一个封闭的环境。它可以联网，也可以和本地文件交互，因此你不需要上传文件进行分析。而且，如果在分析过程中如果它缺少某些 Python 库，可以联网自行下载。
>
> 此外，其他的对于上传文件的大小是有限制的，但 Open Interpreter 却可以和你电脑上任意大的文件进行交互。因此，它对于处理文件非常方便。

# 极简部署安装

> 较为推荐使用此方法+key

Linux上部署

~~~bash
pip install open-interpreter
~~~

Windows上部署

~~~bash
pip install open-interpreter
~~~

mac上部署

~~~bash
pip install open-interpreter
~~~

api的配置

~~~bash
OPENAI_API_KEY
OPENAI_API_BASE
~~~

# 使用本地模型

> 不是很推荐

以ollama举例

首先安装ollama

~~~
Download Ollama - https://ollama.ai/download
~~~

ollama启动某个模型

~~~
ollama run qwen:14b
~~~

本地模型启动

~~~
interpreter --model ollama/qwen:14b
~~~

# 使用Litellm做接口访问

> 次推荐

LiteLLM的功能就一句话：使用 OpenAI 格式调用所有 LLM API [Bedrock、Huggingface、VertexAI、TogetherAI、Azure、OpenAI 等]，具体包括：

- 将输入转换为提供者的`completion`、`embedding`和`image_generation`端点
- [一致的输出](https://link.zhihu.com/?target=https%3A//docs.litellm.ai/docs/completion/output)，文本响应将始终可用`['choices'][0]['message']['content']`
- 跨多个部署（例如 Azure/OpenAI）的重试/回退逻辑 -[路由器](https://link.zhihu.com/?target=https%3A//docs.litellm.ai/docs/routing)
- 设置每个项目的预算和费率限制、API 密钥、模型[OpenAI 代理服务器](https://link.zhihu.com/?target=https%3A//docs.litellm.ai/docs/simple_proxy)

项目地址：[BerriAI/litellm: Call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs) (github.com)](https://github.com/BerriAI/litellm)

# todo 

- [ ] 接入语音
